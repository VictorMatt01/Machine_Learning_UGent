{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical session ML: Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import datasets, neighbors, metrics, tree, svm, preprocessing\n",
    "\n",
    "# Calculates the per-class accuracy given predicted and true output labels.\n",
    "def class_accs(y_pred, y_true):\n",
    "    acc0 = ((y_pred == y_true) & (y_true == 0)).sum() / (y_true == 0).sum()\n",
    "    acc1 = ((y_pred == y_true) & (y_true == 1)).sum() / (y_true == 1).sum()\n",
    "    return acc0, acc1\n",
    "\n",
    "# Prints a summary of performance metrics given predicted and true output labels.\n",
    "def print_metrics(y_pred, y_true):\n",
    "    f1 = metrics.f1_score(y_true, y_pred)\n",
    "    acc = metrics.accuracy_score(y_true, y_pred)\n",
    "    acc0, acc1 = class_accs(y_pred, y_true)\n",
    "    print(f'\\tF1 = {f1}')\n",
    "    print(f'\\tAccuracy = {acc}')\n",
    "    print(f'\\t\\tclass 0: {acc0}')\n",
    "    print(f'\\t\\tclass 1: {acc1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Wisconsin breast cancer\n",
    "The first dataset we will be using is the Wisconsin breast cancer dataset. This dataset contains characteristics of cell nuclei present in medical images of breast mass. The task is to classify these measurements into a positive (\"benign\") or negative (\"malignant\") class. First, we load the data and explore its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set contains 569 instances with 30 features.\n",
      "The different classes are ['malignant' 'benign'].\n",
      "malignant contains 569 samples, benign contains 569 samples.\n"
     ]
    }
   ],
   "source": [
    "# load the Wisconsin breast cancer data set\n",
    "wisconsin = datasets.load_breast_cancer()\n",
    "X = wisconsin[\"data\"]\n",
    "y = wisconsin[\"target\"]\n",
    "names = wisconsin['target_names']\n",
    "\n",
    "# explore the data set\n",
    "print(f\"Data set contains {X.shape[0]} instances with {X.shape[1]} features.\")\n",
    "print(f\"The different classes are {wisconsin['target_names']}.\")\n",
    "print(f\"{names[0]} contains {len(y == 0)} samples, {names[1]} contains {len(y == 1)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wisconsin.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first apply a simple, linear SVM to predict the class of unseen samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tF1 = 0.9696969696969697\n",
      "\tAccuracy = 0.9553903345724907\n",
      "\t\tclass 0: 0.9848484848484849\n",
      "\t\tclass 1: 0.9458128078817734\n"
     ]
    }
   ],
   "source": [
    "X_train = X[:300]\n",
    "y_train = y[:300]\n",
    "X_test = X[300:]\n",
    "y_test = y[300:]\n",
    "\n",
    "sv = svm.SVC(kernel = 'linear')\n",
    "sv.fit(X_train, y_train)\n",
    "y_pred = sv.predict(X_test)\n",
    "print_metrics(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Which of the previous metrics would you use? Can you think of another metric that would reflect the classifier's accuracy on this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: k-fold cross validation\n",
    "\n",
    "A better way to evaluate the performance of a classifier on a dataset is to use k-fold cross validation (e.g. with k=5). In this case, we divide the dataset into k parts (\"folds\"), train the algorithm on (k-1) parts and test it on the final k-th part. We repeat this process k times to get a more representative image of the real performance of the classifier.\n",
    "\n",
    "\n",
    "**Exercise:** Implement a function for k-fold cross validation. Use stratified CV, meaning that the original class distribution is respected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(X, y, folds, clf):\n",
    "    pass  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: comparison of SVM kernels\n",
    "Now use the function you just wrote to compare the performance of SVM on this dataset using a linear kernel vs an rbf kernel. If you weren't able to finish the previous exercise, you can just use the given `print_metrics` function.\n",
    "\n",
    "**Question:** What can you conclude from this experiment? What do you notice regarding the speed of the different algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: comparison of algorithms\n",
    "\n",
    "Compare the performance of kNN, SVM and Decision Trees on the Wisconsin data set. Vary the internal parameters (number of nearest neighbors, impurity measure, kernel, etc.). Consult the [API documentation](http://scikit-learn.org/stable/modules/classes.html) for more details.\n",
    "\n",
    "**Question:** What can we conclude from this experiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: XOR dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now switch to a second, synthetically generated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X_xor = np.random.randn(200, 2)\n",
    "y_xor = np.logical_xor(X_xor[:, 0] > 0,\n",
    "                       X_xor[:, 1] > 0)\n",
    "y_xor = np.where(y_xor, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Apply the same linear SVM that we applied on the wisconsin dataset earlier. Does it work well here? Can you explain why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Can you think of other methods that you could use to classify this dataset? For every method, explain why it would work better on this type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Titanic dataset\n",
    "Another very popular classification dataset is the Titanic dataset, in which we try to predict the survival of passengers on the Titanic, based on their class, age, sex, etc. In this part, we try out a few different algorithms on this dataset.\n",
    "\n",
    "First, we load the data. It has already been converted to vector form (with categorical values converted to dummy representation), and only needs to be normalized. People who are interested in the original dataset can go to https://www.kaggle.com/c/titanic/data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt(\"titanic.csv\", delimiter=\",\")\n",
    "y = np.loadtxt(\"labels.csv\", delimiter=\",\").astype(np.int)\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a number of classification algorithms to the Titanic dataset, and see which ones work best:\n",
    "- KNN (for varying K)\n",
    "- Decision tree\n",
    "- SVM (linear/rbf/poly kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
