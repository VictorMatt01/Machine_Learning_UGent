{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on the following link: https://datascienceplus.com/long-short-term-memory-lstm-and-how-to-implement-lstm-using-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "# creating a random array of int's\n",
    "features = (np.random.randint(10, size=(100, 1)))\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    }
   ],
   "source": [
    "# split the data\n",
    "training_dataset_length = math.ceil(len(features) * .75)\n",
    "print(training_dataset_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data i.e feature scaling to scale the data to be valued in between 0 and 1, which is a good practice to scale the data before feeding it into a neural network for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the all of the data to be values between 0 and 1 \n",
    "scaler = MinMaxScaler(feature_range=(0, 1)) \n",
    "scaled_data = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = scaled_data[0:training_dataset_length  , : ]\n",
    "\n",
    "#Splitting the data\n",
    "x_train=[]\n",
    "y_train = []\n",
    "\n",
    "for i in range(10, len(train_data)):\n",
    "    x_train.append(train_data[i-10:i,0])\n",
    "    y_train.append(train_data[i,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to numpy arrays\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "#Reshape the data into 3-D array\n",
    "x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the architecture\n",
    "* Make an object of the sequential model. Then add the LSTM layer with parameters (units: the dimension of output space, input_shape: the shape of the training set, return_sequences: Ture or False, determines whether to return the last output in the output sequence or the full sequence.\n",
    "* We add 4 of the LSTM layers each with a dropout layer of value(0.2).{Droupout layer is a type of regularization technique which is used to prevent overfitting, but it may also increase training time in some cases.}\n",
    "* Final layer is the output layer which is a fully connected dense layer(units = 1, as we are predicting only one value i.e l+1).{Dense layer performs the operation on the input layers and returns the output and every neuron at the previous layer is connected to the neurons in the next layer hence it is called fully connected Dense layer.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "# Initialising the RNN\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(units = 50, return_sequences = True, input_shape = (x_train.shape[1], 1)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Adding a second LSTM layer and Dropout layer\n",
    "model.add(LSTM(units = 50, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Adding a third LSTM layer and Dropout layer\n",
    "model.add(LSTM(units = 50, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Adding a fourth LSTM layer and and Dropout layer\n",
    "model.add(LSTM(units = 50))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Adding the output layer\n",
    "# For Full connection layer we use dense\n",
    "# As the output is 1D so we use unit=1\n",
    "model.add(Dense(units = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use adam optimizer and the error is calculated by loss function 'mean squared error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.3567\n",
      "Epoch 2/30\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.2862\n",
      "Epoch 3/30\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2161\n",
      "Epoch 4/30\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1379\n",
      "Epoch 5/30\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0884\n",
      "Epoch 6/30\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0996\n",
      "Epoch 7/30\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1144\n",
      "Epoch 8/30\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0883\n",
      "Epoch 9/30\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0880\n",
      "Epoch 10/30\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0803\n",
      "Epoch 11/30\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0899\n",
      "Epoch 12/30\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0886\n",
      "Epoch 13/30\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0813\n",
      "Epoch 14/30\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0808\n",
      "Epoch 15/30\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0795\n",
      "Epoch 16/30\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0833\n",
      "Epoch 17/30\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0815\n",
      "Epoch 18/30\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0843\n",
      "Epoch 19/30\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0823\n",
      "Epoch 20/30\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0857\n",
      "Epoch 21/30\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0803\n",
      "Epoch 22/30\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0814\n",
      "Epoch 23/30\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0850\n",
      "Epoch 24/30\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0790\n",
      "Epoch 25/30\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0826\n",
      "Epoch 26/30\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0807\n",
      "Epoch 27/30\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0796\n",
      "Epoch 28/30\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0832\n",
      "Epoch 29/30\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0828\n",
      "Epoch 30/30\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x144550fd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compile and fit the model on 30 epochs\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "model.fit(x_train, y_train, epochs = 30, batch_size = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create testdata simular to the traindata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test data set\n",
    "test_data = scaled_data[training_dataset_length - 10: , : ]\n",
    "\n",
    "#splitting the x_test and y_test data sets\n",
    "x_test = []\n",
    "y_test =  features[training_dataset_length : , : ] \n",
    "\n",
    "for i in range(10,len(test_data)):\n",
    "    x_test.append(test_data[i-10:i,0])\n",
    "    \n",
    "#Convert x_test to a numpy array \n",
    "x_test = np.array(x_test)\n",
    "\n",
    "#Reshape the data into 3-D array\n",
    "x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1660675116795463"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check predicted values\n",
    "predictions = model.predict(x_test) \n",
    "#Undo scaling\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "#Calculate RMSE score\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! The smaller the rmse score the better that the model has performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
